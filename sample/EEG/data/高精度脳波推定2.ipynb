{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from tkinter import ttk\n",
    "from tkinter import *\n",
    "import random\n",
    "import scipy.signal\n",
    "from scipy import fftpack, hamming\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from time import sleep\n",
    "import cv2\n",
    "\n",
    "# データ抽出\n",
    "def  input(x):\n",
    "    a1, a2, a3, a4, a5, a6, a7, a8, a9,a10, a11, a12, a13, a14, a15, a16, a17, a18, a19, a20, a21, a22, a23, a24, a25, a26, a27, a28, a29, a30, a31, a32, a33, a34, a35, a36, a37, a38, a39, a40, a41, a42, a43, a44, a45 = [x.iloc[250*3*i:256+250*3*i,0:8] for i in range(45)]\n",
    "    return (a1, a2, a3, a4, a5, a6, a7, a8, a9,a10, a11, a12, a13, a14, a15, a16, a17, a18, a19, a20, a21, a22, a23, a24, a25, a26, a27, a28, a29, a30, a31, a32, a33, a34, a35, a36, a37, a38, a39, a40, a41, a42, a43, a44, a45)\n",
    "\n",
    "def  input_tes(x):\n",
    "    a1, a2, a3, a4, a5, a6, a7, a8, a9 = [x.iloc[250*5*i:256+250*5*i,0:8] for i in range(9)]\n",
    "\n",
    "    return (a1, a2, a3, a4, a5, a6, a7, a8, a9)\n",
    "\n",
    "def fft_t01(data, N, fs):\n",
    "    dt = 1 /fs\n",
    "    X = np.fft.fft(data)\n",
    "    freq = np.linspace(0, 1.0/dt, N) # 周波数軸\n",
    "    amplitudeSpectrum = [np.sqrt(c.real ** 2 + c.imag ** 2) for c in X]\n",
    "    n = sum(amplitudeSpectrum)\n",
    "    for i in range(len(amplitudeSpectrum)):\n",
    "        amplitudeSpectrum[i] = amplitudeSpectrum[i]/n\n",
    "\n",
    "    amplitudeSpectrum = np.array(amplitudeSpectrum)\n",
    "    Spectrum =  amplitudeSpectrum[np.logical_and(freq>=3.0, freq<32)]\n",
    "    am_sum = sum(Spectrum)\n",
    "    a =  amplitudeSpectrum[np.logical_and(freq>=4, freq<8)]\n",
    "    a = sum(a)\n",
    "    p = a / am_sum\n",
    "\n",
    "    return(p)\n",
    "\n",
    "def fft_t02(data, N, fs):\n",
    "    dt = 1 /fs\n",
    "    X = np.fft.fft(data)\n",
    "    freq = np.linspace(0, 1.0/dt, N) # 周波数軸\n",
    "    amplitudeSpectrum = [np.sqrt(c.real ** 2 + c.imag ** 2) for c in X]\n",
    "    n = sum(amplitudeSpectrum)\n",
    "    for i in range(len(amplitudeSpectrum)):\n",
    "        amplitudeSpectrum[i] = amplitudeSpectrum[i]/n\n",
    "\n",
    "    amplitudeSpectrum = np.array(amplitudeSpectrum)\n",
    "    Spectrum =  amplitudeSpectrum[np.logical_and(freq>=3.0, freq<32)]\n",
    "    am_sum = sum(Spectrum)\n",
    "    a =  amplitudeSpectrum[np.logical_and(freq>=8, freq<13)]\n",
    "    a = sum(a)\n",
    "    p = a / am_sum\n",
    "\n",
    "    return(p)\n",
    "\n",
    "def fft_t03(data, N, fs):\n",
    "    dt = 1 /fs\n",
    "    X = np.fft.fft(data)\n",
    "    freq = np.linspace(0, 1.0/dt, N) # 周波数軸\n",
    "    amplitudeSpectrum = [np.sqrt(c.real ** 2 + c.imag ** 2) for c in X]\n",
    "    n = sum(amplitudeSpectrum)\n",
    "    for i in range(len(amplitudeSpectrum)):\n",
    "        amplitudeSpectrum[i] = amplitudeSpectrum[i]/n\n",
    "\n",
    "    amplitudeSpectrum = np.array(amplitudeSpectrum)\n",
    "    Spectrum =  amplitudeSpectrum[np.logical_and(freq>=3.0, freq<32)]\n",
    "    am_sum = sum(Spectrum)\n",
    "    a =  amplitudeSpectrum[np.logical_and(freq>=13, freq<20)]\n",
    "    a = sum(a)\n",
    "    p = a / am_sum\n",
    "\n",
    "    return(p)\n",
    "\n",
    "\n",
    "def time_separate(data,i):\n",
    "\n",
    "    data1, data2, data3, data4 = [data[i:i+64] for i in range(0,250,64)]\n",
    "    data_list = [data1, data2, data3, data4]\n",
    "\n",
    "    a1 = data1.iloc[:, i]\n",
    "    a2 = data2.iloc[:, i]\n",
    "    a3 = data3.iloc[:, i]\n",
    "    a4 = data4.iloc[:, i]\n",
    "\n",
    "    x1 = fft_t01(a1,len(a1), 250)\n",
    "    x2 = fft_t01(a2,len(a2), 250)\n",
    "    x3 = fft_t01(a3,len(a3), 250)\n",
    "    x4 = fft_t01(a4,len(a4), 250)\n",
    "\n",
    "    x5 = fft_t02(a1,len(a1), 250)\n",
    "    x6 = fft_t02(a2,len(a2), 250)\n",
    "    x7 = fft_t02(a3,len(a3), 250)\n",
    "    x8 = fft_t02(a4,len(a4), 250)\n",
    "\n",
    "    x9 = fft_t03(a1,len(a1), 250)\n",
    "    x10 = fft_t03(a2,len(a2), 250)\n",
    "    x11 = fft_t03(a3,len(a3), 250)\n",
    "    x12 = fft_t03(a4,len(a4), 250)\n",
    "\n",
    "    return(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12)\n",
    "\n",
    "def get_train(text_data, t, label_list):\n",
    "    colum_name=['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "    aso = pd.read_csv(text_data, index_col=0, header=None, names=colum_name)\n",
    "    aso = aso.ix[6:, :8]\n",
    "    start = int(t*250)\n",
    "    aso = aso.ix[start:, :]\n",
    "\n",
    "    a1, a2, a3, a4, a5, a6, a7, a8, a9,a10, a11, a12, a13, a14, a15, a16, a17, a18, a19, a20, a21, a22, a23, a24, a25, a26, a27, a28, a29, a30, a31, a32, a33, a34, a35, a36, a37, a38, a39, a40, a41, a42, a43, a44, a45 = input(aso)\n",
    "    n = [a1, a2, a3, a4, a5, a6, a7, a8, a9,a10, a11, a12, a13, a14, a15, a16, a17, a18, a19, a20, a21, a22, a23, a24, a25, a26, a27, a28, a29, a30, a31, a32, a33, a34, a35, a36, a37, a38, a39, a40, a41, a42, a43, a44, a45]\n",
    "    features_df = pd.DataFrame()\n",
    "    features_df['labels'] = label_list\n",
    "    for i, j in zip(range(len(n)), n):\n",
    "        x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12 = time_separate(j,0)\n",
    "        features_df.loc[i, 'T1_s_1'] = x1\n",
    "        features_df.loc[i, 'T2_s_1'] = x2\n",
    "        features_df.loc[i, 'T3_s_1'] = x3\n",
    "        features_df.loc[i, 'T4_s_1'] = x4\n",
    "        features_df.loc[i, 'T1_a_1'] = x5\n",
    "        features_df.loc[i, 'T2_a_1'] = x6\n",
    "        features_df.loc[i, 'T3_a_1'] = x7\n",
    "        features_df.loc[i, 'T4_a_1'] = x8\n",
    "        features_df.loc[i, 'T1_b_1'] = x9\n",
    "        features_df.loc[i, 'T2_b_1'] = x10\n",
    "        features_df.loc[i, 'T3_b_1'] = x11\n",
    "        features_df.loc[i, 'T4_b_1'] = x12\n",
    "\n",
    "    for i, j in zip(range(len(n)), n):\n",
    "        x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12 = time_separate(j,1)\n",
    "        features_df.loc[i, 'T1_s_2'] = x1\n",
    "        features_df.loc[i, 'T2_s_2'] = x2\n",
    "        features_df.loc[i, 'T3_s_2'] = x3\n",
    "        features_df.loc[i, 'T4_s_2'] = x4\n",
    "        features_df.loc[i, 'T1_a_2'] = x5\n",
    "        features_df.loc[i, 'T2_a_2'] = x6\n",
    "        features_df.loc[i, 'T3_a_2'] = x7\n",
    "        features_df.loc[i, 'T4_a_2'] = x8\n",
    "        features_df.loc[i, 'T1_b_2'] = x9\n",
    "        features_df.loc[i, 'T2_b_2'] = x10\n",
    "        features_df.loc[i, 'T3_b_2'] = x11\n",
    "        features_df.loc[i, 'T4_b_2'] = x12\n",
    "\n",
    "    for i, j in zip(range(len(n)), n):\n",
    "        x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12 = time_separate(j,2)\n",
    "        features_df.loc[i, 'T1_s_3'] = x1\n",
    "        features_df.loc[i, 'T2_s_3'] = x2\n",
    "        features_df.loc[i, 'T3_s_3'] = x3\n",
    "        features_df.loc[i, 'T4_s_3'] = x4\n",
    "        features_df.loc[i, 'T1_a_3'] = x5\n",
    "        features_df.loc[i, 'T2_a_3'] = x6\n",
    "        features_df.loc[i, 'T3_a_3'] = x7\n",
    "        features_df.loc[i, 'T4_a_3'] = x8\n",
    "        features_df.loc[i, 'T1_b_3'] = x9\n",
    "        features_df.loc[i, 'T2_b_3'] = x10\n",
    "        features_df.loc[i, 'T3_b_3'] = x11\n",
    "        features_df.loc[i, 'T4_b_3'] = x12\n",
    "\n",
    "    for i, j in zip(range(len(n)), n):\n",
    "        x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12 = time_separate(j,3)\n",
    "        features_df.loc[i, 'T1_s_4'] = x1\n",
    "        features_df.loc[i, 'T2_s_4'] = x2\n",
    "        features_df.loc[i, 'T3_s_4'] = x3\n",
    "        features_df.loc[i, 'T4_s_4'] = x4\n",
    "        features_df.loc[i, 'T1_a_4'] = x5\n",
    "        features_df.loc[i, 'T2_a_4'] = x6\n",
    "        features_df.loc[i, 'T3_a_4'] = x7\n",
    "        features_df.loc[i, 'T4_a_4'] = x8\n",
    "        features_df.loc[i, 'T1_b_4'] = x9\n",
    "        features_df.loc[i, 'T2_b_4'] = x10\n",
    "        features_df.loc[i, 'T3_b_4'] = x11\n",
    "        features_df.loc[i, 'T4_b_4'] = x12\n",
    "\n",
    "    for i, j in zip(range(len(n)), n):\n",
    "        x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12 = time_separate(j,4)\n",
    "        features_df.loc[i, 'T1_s_5'] = x1\n",
    "        features_df.loc[i, 'T2_s_5'] = x2\n",
    "        features_df.loc[i, 'T3_s_5'] = x3\n",
    "        features_df.loc[i, 'T4_s_5'] = x4\n",
    "        features_df.loc[i, 'T1_a_5'] = x5\n",
    "        features_df.loc[i, 'T2_a_5'] = x6\n",
    "        features_df.loc[i, 'T3_a_5'] = x7\n",
    "        features_df.loc[i, 'T4_a_5'] = x8\n",
    "        features_df.loc[i, 'T1_b_5'] = x9\n",
    "        features_df.loc[i, 'T2_b_5'] = x10\n",
    "        features_df.loc[i, 'T3_b_5'] = x11\n",
    "        features_df.loc[i, 'T4_b_5'] = x12\n",
    "\n",
    "    for i, j in zip(range(len(n)), n):\n",
    "        x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12 = time_separate(j,5)\n",
    "        features_df.loc[i, 'T1_s_6'] = x1\n",
    "        features_df.loc[i, 'T2_s_6'] = x2\n",
    "        features_df.loc[i, 'T3_s_6'] = x3\n",
    "        features_df.loc[i, 'T4_s_6'] = x4\n",
    "        features_df.loc[i, 'T1_a_6'] = x5\n",
    "        features_df.loc[i, 'T2_a_6'] = x6\n",
    "        features_df.loc[i, 'T3_a_6'] = x7\n",
    "        features_df.loc[i, 'T4_a_6'] = x8\n",
    "        features_df.loc[i, 'T1_b_6'] = x9\n",
    "        features_df.loc[i, 'T2_b_6'] = x10\n",
    "        features_df.loc[i, 'T3_b_6'] = x11\n",
    "        features_df.loc[i, 'T4_b_6'] = x12\n",
    "\n",
    "    for i, j in zip(range(len(n)), n):\n",
    "        x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12 = time_separate(j,6)\n",
    "        features_df.loc[i, 'T1_s_7'] = x1\n",
    "        features_df.loc[i, 'T2_s_7'] = x2\n",
    "        features_df.loc[i, 'T3_s_7'] = x3\n",
    "        features_df.loc[i, 'T4_s_7'] = x4\n",
    "        features_df.loc[i, 'T1_a_7'] = x5\n",
    "        features_df.loc[i, 'T2_a_7'] = x6\n",
    "        features_df.loc[i, 'T3_a_7'] = x7\n",
    "        features_df.loc[i, 'T4_a_7'] = x8\n",
    "        features_df.loc[i, 'T1_b_7'] = x9\n",
    "        features_df.loc[i, 'T2_b_7'] = x10\n",
    "        features_df.loc[i, 'T3_b_7'] = x11\n",
    "        features_df.loc[i, 'T4_b_7'] = x12\n",
    "\n",
    "    for i, j in zip(range(len(n)), n):\n",
    "        x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12 = time_separate(j,7)\n",
    "        features_df.loc[i, 'T1_s_8'] = x1\n",
    "        features_df.loc[i, 'T2_s_8'] = x2\n",
    "        features_df.loc[i, 'T3_s_8'] = x3\n",
    "        features_df.loc[i, 'T4_s_8'] = x4\n",
    "        features_df.loc[i, 'T1_a_8'] = x5\n",
    "        features_df.loc[i, 'T2_a_8'] = x6\n",
    "        features_df.loc[i, 'T3_a_8'] = x7\n",
    "        features_df.loc[i, 'T4_a_8'] = x8\n",
    "        features_df.loc[i, 'T1_b_8'] = x9\n",
    "        features_df.loc[i, 'T2_b_8'] = x10\n",
    "        features_df.loc[i, 'T3_b_8'] = x11\n",
    "        features_df.loc[i, 'T4_b_8'] = x12\n",
    "\n",
    "\n",
    "    #X_train = features_df.ix[:, 1:]\n",
    "    #y_train = features_df.ix[:, 0]\n",
    "\n",
    "    return(features_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/schwarz/.pyenv/versions/3.6.4/lib/python3.6/site-packages/ipykernel_launcher.py:118: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "/Users/schwarz/.pyenv/versions/3.6.4/lib/python3.6/site-packages/ipykernel_launcher.py:120: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n"
     ]
    }
   ],
   "source": [
    "#os.chdir(\"/Applications/SavedData\")\n",
    "#feature_df = get_train(\"aso_train.txt\")\n",
    "list01 = [1,2,0,1,1,0,2,1,2,1,2,0,2,1,0,1,2,0,1,2,1,0,1,0,2,1,0,1,2,1,0,0,1,2,1,0,2,1,0,2,1,2,0,2,1]\n",
    "list02 = [1,2,0,0,2,1,2,1,2,1,0,0,2,1,2,1,2,0,2,1,0,2,1,0,1,2,0,1,0,1,2,1,0,1,0,1,2,1,2,0,0,2,0,1,0]\n",
    "list03 = [1,0,2,1,0,2,2,1,2,0,1,2,0,1,2,1,0,1,0,2,1,0,2,2,1,1,0,0,1,0,2,2,0,1,2,1,2,0,1,2,0,1,2,0,1]\n",
    "\n",
    "feature_df01 = get_train(\"OpenBCI-RAW-ita01.txt\", 6.67, list01)\n",
    "feature_df02 = get_train(\"OpenBCI-RAW-ita02.txt\", 7.14, list02)\n",
    "feature_df03 = get_train(\"OpenBCI-RAW-ita03.txt\", 7.52, list03)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(135, 97)\n"
     ]
    }
   ],
   "source": [
    "train_feature = pd.concat([feature_df01, feature_df02, feature_df03])\n",
    "#train_feature = pd.concat([feature_df01, feature_df02])\n",
    "#train_feature = train_feature.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "\n",
    "print(train_feature.shape)\n",
    "\n",
    "labels = train_feature['labels']\n",
    "labels = np.array(labels.values.flatten())\n",
    "del train_feature['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      T1_s_1    T2_s_1    T3_s_1    T4_s_1    T1_a_1    T2_a_1    T3_a_1  \\\n",
      "0   0.287022  0.009710  0.183148  0.221280  0.103129  0.131193  0.074229   \n",
      "1   0.143683  0.204207  0.191198  0.120523  0.108317  0.153356  0.188031   \n",
      "2   0.106632  0.177912  0.188830  0.048772  0.095701  0.099233  0.091824   \n",
      "3   0.130907  0.211447  0.229594  0.192491  0.061543  0.207152  0.123722   \n",
      "4   0.035323  0.183649  0.096016  0.169982  0.169671  0.124829  0.023756   \n",
      "5   0.183944  0.175736  0.163191  0.085743  0.068292  0.125532  0.122267   \n",
      "6   0.204305  0.047663  0.304170  0.184646  0.097885  0.052545  0.207877   \n",
      "7   0.078792  0.045522  0.177632  0.161968  0.105494  0.159255  0.093698   \n",
      "8   0.247313  0.240297  0.109061  0.239418  0.108072  0.192637  0.160564   \n",
      "9   0.130473  0.175706  0.170644  0.149406  0.122859  0.096841  0.048473   \n",
      "10  0.094395  0.136393  0.212538  0.160344  0.025633  0.041381  0.107289   \n",
      "11  0.093545  0.101595  0.150236  0.216715  0.121153  0.045541  0.136212   \n",
      "12  0.180701  0.141251  0.156678  0.133066  0.061132  0.063436  0.209362   \n",
      "13  0.161684  0.216934  0.144980  0.186918  0.104285  0.097956  0.118126   \n",
      "14  0.210596  0.167458  0.113699  0.119896  0.135107  0.080133  0.106861   \n",
      "15  0.185453  0.252609  0.202488  0.067581  0.110167  0.057947  0.116812   \n",
      "16  0.103716  0.178832  0.202539  0.220737  0.127518  0.185504  0.058962   \n",
      "17  0.165270  0.091217  0.200343  0.180094  0.107077  0.144046  0.266404   \n",
      "18  0.293796  0.200684  0.279690  0.201271  0.126001  0.168628  0.162027   \n",
      "19  0.104671  0.158909  0.128534  0.163189  0.030928  0.033512  0.170573   \n",
      "20  0.071306  0.224426  0.138708  0.154769  0.159781  0.098610  0.143115   \n",
      "21  0.179588  0.172258  0.232491  0.139716  0.145746  0.125281  0.038440   \n",
      "22  0.161998  0.281188  0.017152  0.146972  0.051409  0.075833  0.179287   \n",
      "23  0.090912  0.128274  0.106866  0.151065  0.178028  0.070365  0.072943   \n",
      "24  0.284302  0.146432  0.165307  0.143954  0.029018  0.090092  0.103570   \n",
      "25  0.210119  0.015545  0.167315  0.173279  0.142001  0.063313  0.095933   \n",
      "26  0.102832  0.268488  0.073726  0.217813  0.334272  0.188719  0.096312   \n",
      "27  0.214826  0.182927  0.215404  0.030139  0.106883  0.171359  0.104153   \n",
      "28  0.245928  0.281320  0.116019  0.253607  0.101415  0.047675  0.120724   \n",
      "29  0.179007  0.155700  0.180637  0.153799  0.058064  0.175295  0.073024   \n",
      "..       ...       ...       ...       ...       ...       ...       ...   \n",
      "15  0.113791  0.089806  0.126868  0.114583  0.107436  0.052020  0.115252   \n",
      "16  0.104770  0.106198  0.115668  0.123452  0.118576  0.072740  0.107792   \n",
      "17  0.103259  0.082781  0.100676  0.122310  0.114682  0.070805  0.122272   \n",
      "18  0.144884  0.192332  0.187652  0.120403  0.121220  0.104799  0.116209   \n",
      "19  0.105177  0.133219  0.103450  0.123240  0.109237  0.099083  0.089405   \n",
      "20  0.118186  0.111986  0.215868  0.161285  0.128813  0.124600  0.176270   \n",
      "21  0.179656  0.187037  0.182743  0.164961  0.120723  0.116074  0.131761   \n",
      "22  0.149084  0.175999  0.169635  0.165851  0.105577  0.112401  0.117353   \n",
      "23  0.162934  0.122854  0.136237  0.120206  0.124433  0.097964  0.111908   \n",
      "24  0.121299  0.125491  0.116061  0.119216  0.059520  0.082586  0.123332   \n",
      "25  0.108378  0.115425  0.137093  0.118718  0.113807  0.088749  0.105894   \n",
      "26  0.150929  0.136702  0.112019  0.158726  0.113704  0.092773  0.113234   \n",
      "27  0.179766  0.179385  0.201794  0.222588  0.111315  0.136454  0.136809   \n",
      "28  0.205597  0.121997  0.160148  0.198592  0.110586  0.112459  0.118532   \n",
      "29  0.200824  0.161655  0.135787  0.146967  0.114707  0.073492  0.121971   \n",
      "30  0.132301  0.052459  0.116081  0.123324  0.096553  0.113193  0.101987   \n",
      "31  0.230590  0.164156  0.171973  0.138133  0.127969  0.121508  0.141130   \n",
      "32  0.107869  0.134371  0.058637  0.115797  0.108344  0.087538  0.083323   \n",
      "33  0.142723  0.087431  0.051577  0.102529  0.101179  0.098608  0.077596   \n",
      "34  0.109387  0.104374  0.097262  0.106400  0.115271  0.110331  0.124943   \n",
      "35  0.162075  0.106345  0.138053  0.161652  0.107185  0.115149  0.113996   \n",
      "36  0.118838  0.164102  0.203282  0.076371  0.115630  0.097826  0.087274   \n",
      "37  0.130601  0.100360  0.116316  0.118900  0.113723  0.125182  0.089319   \n",
      "38  0.109757  0.154161  0.097282  0.110786  0.124488  0.069901  0.096014   \n",
      "39  0.127811  0.129069  0.094172  0.100919  0.121000  0.064678  0.105703   \n",
      "40  0.105559  0.103610  0.102324  0.113706  0.095841  0.113088  0.116783   \n",
      "41  0.150883  0.160975  0.070137  0.207446  0.125502  0.120502  0.100576   \n",
      "42  0.200877  0.145010  0.079101  0.075755  0.143448  0.103285  0.119016   \n",
      "43  0.125922  0.125563  0.156608  0.112461  0.090618  0.100299  0.090580   \n",
      "44  0.220428  0.165374  0.376751  0.145493  0.171611  0.115350  0.163827   \n",
      "\n",
      "      T4_a_1    T1_b_1    T2_b_1    ...       T3_s_8    T4_s_8    T1_a_8  \\\n",
      "0   0.033080  0.181379  0.149324    ...     0.113364  0.117334  0.107337   \n",
      "1   0.106139  0.089656  0.108795    ...     0.097026  0.113596  0.115382   \n",
      "2   0.151352  0.096406  0.234131    ...     0.102248  0.117061  0.120797   \n",
      "3   0.131618  0.211967  0.243625    ...     0.115991  0.102974  0.126210   \n",
      "4   0.122088  0.127445  0.091614    ...     0.112349  0.093386  0.102643   \n",
      "5   0.130262  0.164426  0.335406    ...     0.075381  0.108926  0.119676   \n",
      "6   0.097142  0.036237  0.270929    ...     0.107490  0.097998  0.080125   \n",
      "7   0.128602  0.167320  0.308636    ...     0.133297  0.100902  0.125675   \n",
      "8   0.075454  0.161685  0.127778    ...     0.101071  0.121867  0.114654   \n",
      "9   0.340725  0.156287  0.107691    ...     0.106484  0.043674  0.078147   \n",
      "10  0.227603  0.042591  0.071537    ...     0.109820  0.116228  0.114770   \n",
      "11  0.027462  0.257228  0.200104    ...     0.118954  0.101385  0.036865   \n",
      "12  0.112216  0.201792  0.155635    ...     0.067403  0.116250  0.110932   \n",
      "13  0.138188  0.115068  0.257899    ...     0.117025  0.077291  0.094495   \n",
      "14  0.041917  0.119412  0.129506    ...     0.056368  0.120330  0.105727   \n",
      "15  0.050296  0.170373  0.164721    ...     0.101261  0.065687  0.124566   \n",
      "16  0.238477  0.245981  0.130203    ...     0.109115  0.113732  0.118698   \n",
      "17  0.110804  0.153501  0.140189    ...     0.108399  0.098588  0.099269   \n",
      "18  0.121442  0.295416  0.178427    ...     0.107019  0.111737  0.113761   \n",
      "19  0.166158  0.214470  0.144965    ...     0.122644  0.097896  0.088933   \n",
      "20  0.078316  0.261073  0.080201    ...     0.087806  0.121428  0.108710   \n",
      "21  0.166446  0.154291  0.154885    ...     0.117943  0.088004  0.046395   \n",
      "22  0.102480  0.310276  0.234789    ...     0.090127  0.115566  0.107470   \n",
      "23  0.071156  0.202771  0.169770    ...     0.116025  0.035236  0.029678   \n",
      "24  0.065877  0.113239  0.128086    ...     0.095879  0.115702  0.112756   \n",
      "25  0.134250  0.190020  0.083274    ...     0.099376  0.059676  0.089443   \n",
      "26  0.147652  0.175091  0.082586    ...     0.109294  0.109940  0.108265   \n",
      "27  0.084597  0.072734  0.233611    ...     0.113738  0.111246  0.109620   \n",
      "28  0.088686  0.064926  0.143987    ...     0.077501  0.096946  0.098236   \n",
      "29  0.079759  0.214406  0.187759    ...     0.108084  0.096316  0.117473   \n",
      "..       ...       ...       ...    ...          ...       ...       ...   \n",
      "15  0.111485  0.217829  0.252746    ...     0.099485  0.110653  0.112635   \n",
      "16  0.118972  0.250174  0.174242    ...     0.129600  0.167975  0.109641   \n",
      "17  0.113837  0.237165  0.235878    ...     0.089359  0.116040  0.103912   \n",
      "18  0.145496  0.181676  0.175927    ...     0.093330  0.111866  0.111035   \n",
      "19  0.122119  0.232953  0.206312    ...     0.077785  0.098747  0.102129   \n",
      "20  0.092928  0.235127  0.223291    ...     0.120938  0.091291  0.137573   \n",
      "21  0.112893  0.159014  0.176377    ...     0.120298  0.064472  0.112254   \n",
      "22  0.139432  0.144251  0.163540    ...     0.127696  0.095287  0.029165   \n",
      "23  0.091028  0.164740  0.185104    ...     0.110789  0.116753  0.084712   \n",
      "24  0.125671  0.233023  0.170993    ...     0.080092  0.120844  0.098542   \n",
      "25  0.106590  0.239860  0.170179    ...     0.151112  0.125054  0.103035   \n",
      "26  0.104996  0.180953  0.199260    ...     0.107013  0.109973  0.116185   \n",
      "27  0.108464  0.164491  0.145849    ...     0.169178  0.176017  0.085744   \n",
      "28  0.107529  0.134832  0.180222    ...     0.110480  0.105818  0.075510   \n",
      "29  0.106776  0.128465  0.198166    ...     0.111990  0.112807  0.091602   \n",
      "30  0.112108  0.236986  0.225188    ...     0.110655  0.106135  0.123474   \n",
      "31  0.148581  0.179570  0.123935    ...     0.061984  0.112607  0.115686   \n",
      "32  0.115240  0.220073  0.232936    ...     0.093522  0.085260  0.121361   \n",
      "33  0.110324  0.237512  0.246143    ...     0.091454  0.090849  0.118838   \n",
      "34  0.127695  0.223727  0.225565    ...     0.113154  0.006771  0.108894   \n",
      "35  0.131150  0.171538  0.273934    ...     0.121506  0.094103  0.121432   \n",
      "36  0.111238  0.247609  0.178626    ...     0.089078  0.078145  0.114464   \n",
      "37  0.126746  0.239009  0.240103    ...     0.072504  0.132528  0.109534   \n",
      "38  0.105327  0.244573  0.235495    ...     0.045976  0.127259  0.118916   \n",
      "39  0.112884  0.265106  0.205766    ...     0.104977  0.111474  0.108332   \n",
      "40  0.087415  0.197115  0.206307    ...     0.103852  0.107352  0.130179   \n",
      "41  0.112767  0.172402  0.158275    ...     0.120693  0.103332  0.114080   \n",
      "42  0.161402  0.180015  0.221411    ...     0.116196  0.095965  0.102923   \n",
      "43  0.114521  0.245591  0.176892    ...     0.120653  0.074483  0.107595   \n",
      "44  0.117668  0.187862  0.171989    ...     0.124763  0.136808  0.114387   \n",
      "\n",
      "      T2_a_8    T3_a_8    T4_a_8    T1_b_8    T2_b_8    T3_b_8    T4_b_8  \n",
      "0   0.090320  0.120558  0.105280  0.226321  0.233681  0.243147  0.226008  \n",
      "1   0.117434  0.103121  0.091704  0.232053  0.247744  0.265074  0.193041  \n",
      "2   0.108904  0.099026  0.119743  0.240784  0.228139  0.218663  0.232515  \n",
      "3   0.110122  0.126481  0.116693  0.245973  0.231004  0.240902  0.229556  \n",
      "4   0.120504  0.095825  0.070979  0.259934  0.238502  0.236139  0.238673  \n",
      "5   0.105323  0.112000  0.123977  0.231985  0.224147  0.251953  0.254490  \n",
      "6   0.102781  0.107485  0.098294  0.217990  0.252063  0.248346  0.215069  \n",
      "7   0.111752  0.119276  0.107841  0.243405  0.228453  0.249536  0.221650  \n",
      "8   0.040753  0.131264  0.115651  0.227926  0.157093  0.246202  0.251494  \n",
      "9   0.107126  0.110896  0.074732  0.227938  0.227946  0.234579  0.205657  \n",
      "10  0.113176  0.084102  0.125389  0.223809  0.224265  0.214669  0.226543  \n",
      "11  0.122066  0.128838  0.097555  0.274641  0.231337  0.237943  0.211444  \n",
      "12  0.108016  0.086112  0.106997  0.241420  0.236968  0.206678  0.244735  \n",
      "13  0.109097  0.097404  0.093735  0.221275  0.218012  0.244603  0.250434  \n",
      "14  0.111442  0.096573  0.101523  0.248864  0.230112  0.236030  0.222688  \n",
      "15  0.114187  0.122833  0.086370  0.234552  0.238003  0.242456  0.276330  \n",
      "16  0.107496  0.098069  0.108200  0.231305  0.240158  0.212784  0.237745  \n",
      "17  0.118920  0.121742  0.089803  0.245458  0.232054  0.237113  0.233962  \n",
      "18  0.111840  0.088958  0.117217  0.250855  0.234007  0.236161  0.244663  \n",
      "19  0.101937  0.106854  0.101138  0.217580  0.236412  0.246346  0.246572  \n",
      "20  0.121351  0.078391  0.117066  0.249122  0.243413  0.193784  0.240822  \n",
      "21  0.127809  0.104737  0.061612  0.206569  0.239829  0.241878  0.162018  \n",
      "22  0.117635  0.117689  0.111904  0.244940  0.246097  0.235273  0.241169  \n",
      "23  0.118627  0.115517  0.097783  0.283131  0.240208  0.239191  0.209953  \n",
      "24  0.122488  0.106373  0.114124  0.242071  0.212921  0.226169  0.248084  \n",
      "25  0.107805  0.111958  0.021458  0.210312  0.236072  0.250796  0.213111  \n",
      "26  0.098951  0.102919  0.116937  0.250211  0.210643  0.227410  0.229216  \n",
      "27  0.058873  0.109404  0.110016  0.243407  0.255954  0.255723  0.246384  \n",
      "28  0.128220  0.100886  0.121874  0.251004  0.228040  0.242593  0.244734  \n",
      "29  0.111595  0.116238  0.122907  0.234324  0.212220  0.249820  0.241347  \n",
      "..       ...       ...       ...       ...       ...       ...       ...  \n",
      "15  0.122730  0.105402  0.109963  0.237519  0.226372  0.230619  0.252138  \n",
      "16  0.104001  0.071397  0.113768  0.251622  0.204351  0.202810  0.189974  \n",
      "17  0.086519  0.124804  0.112166  0.248519  0.242399  0.239477  0.251634  \n",
      "18  0.107741  0.114853  0.120163  0.232374  0.216889  0.185115  0.235376  \n",
      "19  0.117624  0.086057  0.129154  0.244219  0.239791  0.214255  0.235499  \n",
      "20  0.116163  0.087809  0.098780  0.260039  0.240899  0.186931  0.215443  \n",
      "21  0.111286  0.105646  0.071376  0.188250  0.252726  0.229561  0.213592  \n",
      "22  0.117457  0.099557  0.118038  0.069836  0.227942  0.235533  0.245509  \n",
      "23  0.109767  0.115723  0.112672  0.233662  0.226111  0.237679  0.230990  \n",
      "24  0.071878  0.107910  0.105673  0.253719  0.226514  0.247362  0.244073  \n",
      "25  0.122906  0.106434  0.128879  0.247771  0.183134  0.183828  0.213806  \n",
      "26  0.112520  0.088115  0.063392  0.247337  0.235697  0.240801  0.124379  \n",
      "27  0.112031  0.115504  0.122057  0.146401  0.237407  0.158684  0.169570  \n",
      "28  0.110505  0.113619  0.105912  0.215449  0.219203  0.247922  0.250290  \n",
      "29  0.111284  0.112375  0.118292  0.252889  0.220588  0.226048  0.255238  \n",
      "30  0.109037  0.102833  0.100324  0.230536  0.237474  0.203070  0.257050  \n",
      "31  0.115512  0.069896  0.112748  0.244348  0.241044  0.203425  0.241206  \n",
      "32  0.107075  0.117071  0.091255  0.220187  0.247332  0.231963  0.261419  \n",
      "33  0.118691  0.114808  0.110276  0.242861  0.253096  0.235379  0.243480  \n",
      "34  0.124451  0.125742  0.087293  0.235901  0.235618  0.223168  0.241051  \n",
      "35  0.118799  0.107952  0.094408  0.229482  0.226312  0.212515  0.176774  \n",
      "36  0.109683  0.127176  0.099001  0.245879  0.240537  0.213948  0.257112  \n",
      "37  0.114991  0.045625  0.107376  0.244440  0.248591  0.170816  0.216791  \n",
      "38  0.127579  0.089875  0.116764  0.247921  0.228796  0.260776  0.234422  \n",
      "39  0.095818  0.091086  0.106885  0.254318  0.260635  0.253717  0.240976  \n",
      "40  0.028829  0.125554  0.113827  0.229014  0.270591  0.234822  0.231069  \n",
      "41  0.115846  0.110131  0.125097  0.212665  0.219895  0.238019  0.255576  \n",
      "42  0.119182  0.121101  0.107835  0.175609  0.219903  0.238344  0.181636  \n",
      "43  0.120028  0.099055  0.128919  0.256642  0.239533  0.219807  0.220718  \n",
      "44  0.115867  0.086181  0.103433  0.236409  0.212240  0.170770  0.179558  \n",
      "\n",
      "[135 rows x 96 columns]\n",
      "(135, 96)\n"
     ]
    }
   ],
   "source": [
    "print(train_feature)\n",
    "print(train_feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0         1         2         3         4         5         6   \\\n",
      "90  -0.081945  0.164558  0.069338  0.006667  0.051828  0.003964  0.035650   \n",
      "91  -0.095087 -0.052366 -0.033312  0.060347  0.030277  0.001106 -0.019448   \n",
      "92  -0.097107 -0.062623 -0.018224 -0.044585 -0.097929 -0.030123  0.044702   \n",
      "93   0.009678  0.078680  0.098878 -0.036034  0.112373  0.029585  0.014297   \n",
      "94  -0.050566 -0.016675 -0.185937  0.066305  0.007416  0.013466 -0.035577   \n",
      "95  -0.050920 -0.049192 -0.048836 -0.045797 -0.011948 -0.059822 -0.017725   \n",
      "96  -0.004842  0.098777  0.102120 -0.032478  0.000147  0.048864  0.009777   \n",
      "97  -0.037169  0.005814 -0.071590  0.010201  0.022063 -0.009723 -0.027133   \n",
      "98  -0.105515 -0.057019 -0.017881 -0.018475 -0.019598  0.004640  0.014679   \n",
      "99  -0.115542 -0.043756 -0.006104 -0.027655 -0.053475 -0.030213  0.020554   \n",
      "100 -0.134532  0.000267  0.030302 -0.011835  0.010153 -0.036889 -0.030194   \n",
      "101 -0.099534  0.053352  0.039201  0.002432  0.042181  0.005017 -0.039247   \n",
      "102 -0.115699  0.000521  0.025012 -0.029930  0.030235  0.019299 -0.036290   \n",
      "103 -0.097157  0.025330  0.076493  0.013403  0.041560 -0.015198 -0.003131   \n",
      "104 -0.093606  0.014154  0.104272  0.009143  0.018050 -0.019497 -0.060952   \n",
      "\n",
      "           7         8         9     ...           50        51        52  \\\n",
      "90  -0.015466  0.057936 -0.017706    ...     0.007596 -0.003623  0.003836   \n",
      "91   0.024466  0.008680  0.008219    ...     0.025080  0.012630  0.016223   \n",
      "92  -0.036110 -0.019862  0.085893    ...     0.008977  0.005250 -0.001308   \n",
      "93   0.041636 -0.003606 -0.005083    ...     0.020425  0.035019 -0.005740   \n",
      "94   0.042342  0.032187  0.005781    ...    -0.005672  0.009340 -0.002002   \n",
      "95  -0.043399 -0.035798  0.035724    ...     0.007889  0.026759 -0.017510   \n",
      "96  -0.006645 -0.006036  0.014644    ...     0.002635 -0.047551 -0.009694   \n",
      "97   0.057843 -0.066522 -0.013380    ...    -0.018501 -0.002361  0.000442   \n",
      "98  -0.002763 -0.027791 -0.014805    ...    -0.008035  0.013032 -0.004649   \n",
      "99  -0.038363 -0.034642  0.117412    ...     0.008105  0.018684  0.015163   \n",
      "100 -0.045588  0.054666 -0.003378    ...     0.009981 -0.013639 -0.000325   \n",
      "101 -0.041794  0.045476 -0.018841    ...     0.030301 -0.003483 -0.002160   \n",
      "102 -0.044211  0.043708 -0.053195    ...     0.004746 -0.011587 -0.030598   \n",
      "103 -0.017142  0.023021  0.001682    ...     0.027671 -0.005734 -0.018044   \n",
      "104 -0.091463  0.086426 -0.049898    ...     0.017139 -0.013108 -0.012026   \n",
      "\n",
      "           53        54        55        56        57        58        59  \n",
      "90   0.012584 -0.004244 -0.010965  0.028824  0.010842 -0.014450  0.004838  \n",
      "91  -0.000473 -0.000391 -0.010520 -0.006775 -0.013857 -0.019175  0.000840  \n",
      "92   0.017364 -0.018948 -0.000833 -0.000311 -0.003969 -0.005289 -0.005646  \n",
      "93  -0.006924  0.000290 -0.001156  0.039618 -0.005800  0.015939 -0.005861  \n",
      "94  -0.018111 -0.007383 -0.014253 -0.006752 -0.005338  0.001518  0.012122  \n",
      "95  -0.005408  0.000488  0.022384 -0.019787 -0.000407  0.010249 -0.011290  \n",
      "96  -0.001286 -0.028938  0.013942 -0.031291  0.004092  0.001605 -0.016741  \n",
      "97  -0.026704  0.009522  0.007194 -0.036596 -0.023629  0.056536  0.006883  \n",
      "98  -0.001439  0.006288 -0.015555 -0.005043  0.009055  0.005785 -0.001983  \n",
      "99  -0.010777  0.018218 -0.007306  0.000908 -0.011922 -0.028604 -0.005553  \n",
      "100 -0.017312  0.015785  0.002373 -0.000967  0.000706 -0.016767  0.008607  \n",
      "101 -0.013020  0.009257 -0.011562 -0.028182 -0.010406  0.001261 -0.003079  \n",
      "102 -0.012120  0.003904 -0.014200  0.021447  0.013015  0.006552  0.038104  \n",
      "103 -0.022110  0.001987 -0.004477  0.006681  0.010987 -0.008704  0.023038  \n",
      "104  0.014056  0.020418 -0.009036  0.017456 -0.012944 -0.005927  0.019043  \n",
      "\n",
      "[15 rows x 60 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/schwarz/.pyenv/versions/3.6.4/lib/python3.6/site-packages/ipykernel_launcher.py:18: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n"
     ]
    }
   ],
   "source": [
    "#pca = PCA(n_components=45)\n",
    "pca_data = pd.concat([feature_df01, feature_df02])\n",
    "labels = pca_data['labels']\n",
    "labels = np.array(labels.values.flatten())\n",
    "del pca_data['labels']\n",
    "\n",
    "pca = PCA(n_components=45)\n",
    "#pca.fit(train_feature)\n",
    "#pca.fit(pca_data)\n",
    "#train_feature = pca.transform(train_feature)\n",
    "#train_feature = pca.transform(pd.concat([feature_df01, feature_df02, feature_df03]))\n",
    "#print('累積寄与率: {0}'.format(sum(pca.explained_variance_ratio_)))\n",
    "\n",
    "train_feature = pd.DataFrame(train_feature)\n",
    "\n",
    "#X_train = train_feature.ix[:89, :]\n",
    "#y_train = labels[:90]\n",
    "X_train = train_feature.ix[:89, :]\n",
    "y_train = labels[:90]\n",
    "\n",
    "#X_test1 = train_feature.ix[90:104, :]\n",
    "#y_test1 = labels[90:105]\n",
    "X_test1 = train_feature.ix[90:104, :]\n",
    "y_test1 = labels[90:105]\n",
    "#X_test1 = X_train\n",
    "#y_test1 = y_train\n",
    "print(X_test1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "累積寄与率: 1.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [0, 15]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-528b0a41073e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mforest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mw1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maverage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'micro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mf1_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[1;32m    712\u001b[0m     return fbeta_score(y_true, y_pred, 1, labels=labels,\n\u001b[1;32m    713\u001b[0m                        \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m                        sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mfbeta_score\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[1;32m    826\u001b[0m                                                  \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'f-score'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m                                                  sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    829\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"beta should be >0 in the F-beta score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1025\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1026\u001b[0m     \u001b[0mpresent_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \"\"\"\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 204\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [0, 15]"
     ]
    }
   ],
   "source": [
    "lda = LDA(n_components=2)\n",
    "lda.fit(X_train, y_train)\n",
    "X_train = lda.transform(X_train)\n",
    "X_test1 = lda.transform(X_test1)\n",
    "\n",
    "print('累積寄与率: {0}'.format(sum(lda.explained_variance_ratio_)))\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=200, min_samples_split=15, max_depth=3, max_features=None)\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "w1 = f1_score(y_test1, forest.predict(X_test1),average = 'micro')\n",
    "\n",
    "\n",
    "print(w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 0 1 1 0 2 1 2 1 2 0 2 1 0 1 2 0 1 2 1 0 1 0 2 1 0 1 2 1 0 0 1 2 1 0 2\n",
      " 1 0 2 1 2 0 2 1 1 2 0 0 2 1 2 1 2 1 0 0 2 1 2 1 2 0 2 1 0 2 1 0 1 2 0 1 0\n",
      " 1 2 1 0 1 0 1 2 1 2 0 0 2 0 1 0]\n",
      "[1 2 0 1 1 0 2 1 2 1 2 0 2 1 0 1 2 0 1 2 1 0 1 0 2 1 0 1 2 1 0 0 1 2 1 0 2\n",
      " 1 0 2 1 2 0 2 1 1 2 0 0 2 1 2 1 2 1 2 0 2 1 2 1 2 0 2 1 0 1 1 0 1 2 0 2 0\n",
      " 1 2 1 0 1 0 1 2 1 2 0 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)\n",
    "print(forest.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[27,  0,  1],\n",
       "       [ 0, 33,  1],\n",
       "       [ 1,  1, 26]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_train,forest.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[2 0 0 0 0 0 1 0 0 0 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(y_test1)\n",
    "print(forest.predict(X_test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [0, 15]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-96cdddeb6ddf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mforest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \"\"\"\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \"\"\"\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 204\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [0, 15]"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test1,forest.predict(X_test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_cm = cm[0][0]+cm[1][1]+cm[2][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (tr_cm)/(cm[0][0] + cm[0][1] + cm[0][2] + cm[1][0] + cm[1][1] + cm[1][2] + cm[2][0] + cm[2][1] + cm[2][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

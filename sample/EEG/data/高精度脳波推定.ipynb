{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from tkinter import ttk\n",
    "from tkinter import *\n",
    "import random\n",
    "import scipy.signal\n",
    "from scipy import fftpack, hamming\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from time import sleep\n",
    "import cv2\n",
    "\n",
    "# データ抽出\n",
    "def  input(x):\n",
    "    a1, a2, a3, a4, a5, a6, a7, a8, a9,a10, a11, a12, a13, a14, a15, a16, a17, a18, a19, a20, a21, a22, a23, a24, a25, a26, a27, a28, a29, a30, a31, a32, a33, a34, a35, a36, a37, a38, a39, a40, a41, a42, a43, a44, a45 = [x.iloc[250*3*i:256+250*3*i,0:8] for i in range(45)]\n",
    "    return (a1, a2, a3, a4, a5, a6, a7, a8, a9,a10, a11, a12, a13, a14, a15, a16, a17, a18, a19, a20, a21, a22, a23, a24, a25, a26, a27, a28, a29, a30, a31, a32, a33, a34, a35, a36, a37, a38, a39, a40, a41, a42, a43, a44, a45)\n",
    "\n",
    "def  input_tes(x):\n",
    "    a1, a2, a3, a4, a5, a6, a7, a8, a9 = [x.iloc[250*5*i:256+250*5*i,0:8] for i in range(9)]\n",
    "\n",
    "    return (a1, a2, a3, a4, a5, a6, a7, a8, a9)\n",
    "\n",
    "def fft_t01(data, N, fs):\n",
    "    dt = 1 /fs\n",
    "    X = np.fft.fft(data)\n",
    "    freq = np.linspace(0, 1.0/dt, N) # 周波数軸\n",
    "    amplitudeSpectrum = [np.sqrt(c.real ** 2 + c.imag ** 2) for c in X]\n",
    "    n = sum(amplitudeSpectrum)\n",
    "    for i in range(len(amplitudeSpectrum)):\n",
    "        amplitudeSpectrum[i] = amplitudeSpectrum[i]/n\n",
    "\n",
    "    amplitudeSpectrum = np.array(amplitudeSpectrum)\n",
    "    Spectrum =  amplitudeSpectrum[np.logical_and(freq>=3.0, freq<32)]\n",
    "    am_sum = sum(Spectrum)\n",
    "    a =  amplitudeSpectrum[np.logical_and(freq>=4, freq<8)]\n",
    "    a = sum(a)\n",
    "    p = a / am_sum\n",
    "\n",
    "    return(p)\n",
    "\n",
    "def fft_t02(data, N, fs):\n",
    "    dt = 1 /fs\n",
    "    X = np.fft.fft(data)\n",
    "    freq = np.linspace(0, 1.0/dt, N) # 周波数軸\n",
    "    amplitudeSpectrum = [np.sqrt(c.real ** 2 + c.imag ** 2) for c in X]\n",
    "    n = sum(amplitudeSpectrum)\n",
    "    for i in range(len(amplitudeSpectrum)):\n",
    "        amplitudeSpectrum[i] = amplitudeSpectrum[i]/n\n",
    "\n",
    "    amplitudeSpectrum = np.array(amplitudeSpectrum)\n",
    "    Spectrum =  amplitudeSpectrum[np.logical_and(freq>=3.0, freq<32)]\n",
    "    am_sum = sum(Spectrum)\n",
    "    a =  amplitudeSpectrum[np.logical_and(freq>=8, freq<13)]\n",
    "    a = sum(a)\n",
    "    p = a / am_sum\n",
    "\n",
    "    return(p)\n",
    "\n",
    "def fft_t03(data, N, fs):\n",
    "    dt = 1 /fs\n",
    "    X = np.fft.fft(data)\n",
    "    freq = np.linspace(0, 1.0/dt, N) # 周波数軸\n",
    "    amplitudeSpectrum = [np.sqrt(c.real ** 2 + c.imag ** 2) for c in X]\n",
    "    n = sum(amplitudeSpectrum)\n",
    "    for i in range(len(amplitudeSpectrum)):\n",
    "        amplitudeSpectrum[i] = amplitudeSpectrum[i]/n\n",
    "\n",
    "    amplitudeSpectrum = np.array(amplitudeSpectrum)\n",
    "    Spectrum =  amplitudeSpectrum[np.logical_and(freq>=3.0, freq<32)]\n",
    "    am_sum = sum(Spectrum)\n",
    "    a =  amplitudeSpectrum[np.logical_and(freq>=13, freq<20)]\n",
    "    a = sum(a)\n",
    "    p = a / am_sum\n",
    "\n",
    "    return(p)\n",
    "\n",
    "\n",
    "def time_separate(data,i):\n",
    "\n",
    "    data1, data2, data3, data4 = [data[i:i+64] for i in range(0,250,64)]\n",
    "    data_list = [data1, data2, data3, data4]\n",
    "\n",
    "    a1 = data1.iloc[:, i]\n",
    "    a2 = data2.iloc[:, i]\n",
    "    a3 = data3.iloc[:, i]\n",
    "    a4 = data4.iloc[:, i]\n",
    "\n",
    "    x1 = fft_t01(a1,len(a1), 250)\n",
    "    x2 = fft_t01(a2,len(a2), 250)\n",
    "    x3 = fft_t01(a3,len(a3), 250)\n",
    "    x4 = fft_t01(a4,len(a4), 250)\n",
    "\n",
    "    x5 = fft_t02(a1,len(a1), 250)\n",
    "    x6 = fft_t02(a2,len(a2), 250)\n",
    "    x7 = fft_t02(a3,len(a3), 250)\n",
    "    x8 = fft_t02(a4,len(a4), 250)\n",
    "\n",
    "    x9 = fft_t03(a1,len(a1), 250)\n",
    "    x10 = fft_t03(a2,len(a2), 250)\n",
    "    x11 = fft_t03(a3,len(a3), 250)\n",
    "    x12 = fft_t03(a4,len(a4), 250)\n",
    "\n",
    "    return(x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12)\n",
    "\n",
    "def get_train(text_data, t, label_list):\n",
    "    colum_name=['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "    aso = pd.read_csv(text_data, index_col=0, header=None, names=colum_name)\n",
    "    aso = aso.ix[6:, :8]\n",
    "    start = int(t*250)\n",
    "    aso = aso.ix[start:, :]\n",
    "\n",
    "    a1, a2, a3, a4, a5, a6, a7, a8, a9,a10, a11, a12, a13, a14, a15, a16, a17, a18, a19, a20, a21, a22, a23, a24, a25, a26, a27, a28, a29, a30, a31, a32, a33, a34, a35, a36, a37, a38, a39, a40, a41, a42, a43, a44, a45 = input(aso)\n",
    "    n = [a1, a2, a3, a4, a5, a6, a7, a8, a9,a10, a11, a12, a13, a14, a15, a16, a17, a18, a19, a20, a21, a22, a23, a24, a25, a26, a27, a28, a29, a30, a31, a32, a33, a34, a35, a36, a37, a38, a39, a40, a41, a42, a43, a44, a45]\n",
    "    features_df = pd.DataFrame()\n",
    "    features_df['labels'] = label_list\n",
    "    for i, j in zip(range(len(n)), n):\n",
    "        x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12 = time_separate(j,0)\n",
    "        features_df.loc[i, 'T1_s_1'] = x1\n",
    "        features_df.loc[i, 'T2_s_1'] = x2\n",
    "        features_df.loc[i, 'T3_s_1'] = x3\n",
    "        features_df.loc[i, 'T4_s_1'] = x4\n",
    "        features_df.loc[i, 'T1_a_1'] = x5\n",
    "        features_df.loc[i, 'T2_a_1'] = x6\n",
    "        features_df.loc[i, 'T3_a_1'] = x7\n",
    "        features_df.loc[i, 'T4_a_1'] = x8\n",
    "        features_df.loc[i, 'T1_b_1'] = x9\n",
    "        features_df.loc[i, 'T2_b_1'] = x10\n",
    "        features_df.loc[i, 'T3_b_1'] = x11\n",
    "        features_df.loc[i, 'T4_b_1'] = x12\n",
    "\n",
    "    for i, j in zip(range(len(n)), n):\n",
    "        x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12 = time_separate(j,1)\n",
    "        features_df.loc[i, 'T1_s_2'] = x1\n",
    "        features_df.loc[i, 'T2_s_2'] = x2\n",
    "        features_df.loc[i, 'T3_s_2'] = x3\n",
    "        features_df.loc[i, 'T4_s_2'] = x4\n",
    "        features_df.loc[i, 'T1_a_2'] = x5\n",
    "        features_df.loc[i, 'T2_a_2'] = x6\n",
    "        features_df.loc[i, 'T3_a_2'] = x7\n",
    "        features_df.loc[i, 'T4_a_2'] = x8\n",
    "        features_df.loc[i, 'T1_b_2'] = x9\n",
    "        features_df.loc[i, 'T2_b_2'] = x10\n",
    "        features_df.loc[i, 'T3_b_2'] = x11\n",
    "        features_df.loc[i, 'T4_b_2'] = x12\n",
    "\n",
    "    for i, j in zip(range(len(n)), n):\n",
    "        x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12 = time_separate(j,2)\n",
    "        features_df.loc[i, 'T1_s_3'] = x1\n",
    "        features_df.loc[i, 'T2_s_3'] = x2\n",
    "        features_df.loc[i, 'T3_s_3'] = x3\n",
    "        features_df.loc[i, 'T4_s_3'] = x4\n",
    "        features_df.loc[i, 'T1_a_3'] = x5\n",
    "        features_df.loc[i, 'T2_a_3'] = x6\n",
    "        features_df.loc[i, 'T3_a_3'] = x7\n",
    "        features_df.loc[i, 'T4_a_3'] = x8\n",
    "        features_df.loc[i, 'T1_b_3'] = x9\n",
    "        features_df.loc[i, 'T2_b_3'] = x10\n",
    "        features_df.loc[i, 'T3_b_3'] = x11\n",
    "        features_df.loc[i, 'T4_b_3'] = x12\n",
    "\n",
    "    for i, j in zip(range(len(n)), n):\n",
    "        x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12 = time_separate(j,3)\n",
    "        features_df.loc[i, 'T1_s_4'] = x1\n",
    "        features_df.loc[i, 'T2_s_4'] = x2\n",
    "        features_df.loc[i, 'T3_s_4'] = x3\n",
    "        features_df.loc[i, 'T4_s_4'] = x4\n",
    "        features_df.loc[i, 'T1_a_4'] = x5\n",
    "        features_df.loc[i, 'T2_a_4'] = x6\n",
    "        features_df.loc[i, 'T3_a_4'] = x7\n",
    "        features_df.loc[i, 'T4_a_4'] = x8\n",
    "        features_df.loc[i, 'T1_b_4'] = x9\n",
    "        features_df.loc[i, 'T2_b_4'] = x10\n",
    "        features_df.loc[i, 'T3_b_4'] = x11\n",
    "        features_df.loc[i, 'T4_b_4'] = x12\n",
    "\n",
    "    for i, j in zip(range(len(n)), n):\n",
    "        x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12 = time_separate(j,4)\n",
    "        features_df.loc[i, 'T1_s_5'] = x1\n",
    "        features_df.loc[i, 'T2_s_5'] = x2\n",
    "        features_df.loc[i, 'T3_s_5'] = x3\n",
    "        features_df.loc[i, 'T4_s_5'] = x4\n",
    "        features_df.loc[i, 'T1_a_5'] = x5\n",
    "        features_df.loc[i, 'T2_a_5'] = x6\n",
    "        features_df.loc[i, 'T3_a_5'] = x7\n",
    "        features_df.loc[i, 'T4_a_5'] = x8\n",
    "        features_df.loc[i, 'T1_b_5'] = x9\n",
    "        features_df.loc[i, 'T2_b_5'] = x10\n",
    "        features_df.loc[i, 'T3_b_5'] = x11\n",
    "        features_df.loc[i, 'T4_b_5'] = x12\n",
    "\n",
    "    for i, j in zip(range(len(n)), n):\n",
    "        x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12 = time_separate(j,5)\n",
    "        features_df.loc[i, 'T1_s_6'] = x1\n",
    "        features_df.loc[i, 'T2_s_6'] = x2\n",
    "        features_df.loc[i, 'T3_s_6'] = x3\n",
    "        features_df.loc[i, 'T4_s_6'] = x4\n",
    "        features_df.loc[i, 'T1_a_6'] = x5\n",
    "        features_df.loc[i, 'T2_a_6'] = x6\n",
    "        features_df.loc[i, 'T3_a_6'] = x7\n",
    "        features_df.loc[i, 'T4_a_6'] = x8\n",
    "        features_df.loc[i, 'T1_b_6'] = x9\n",
    "        features_df.loc[i, 'T2_b_6'] = x10\n",
    "        features_df.loc[i, 'T3_b_6'] = x11\n",
    "        features_df.loc[i, 'T4_b_6'] = x12\n",
    "\n",
    "    for i, j in zip(range(len(n)), n):\n",
    "        x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12 = time_separate(j,6)\n",
    "        features_df.loc[i, 'T1_s_7'] = x1\n",
    "        features_df.loc[i, 'T2_s_7'] = x2\n",
    "        features_df.loc[i, 'T3_s_7'] = x3\n",
    "        features_df.loc[i, 'T4_s_7'] = x4\n",
    "        features_df.loc[i, 'T1_a_7'] = x5\n",
    "        features_df.loc[i, 'T2_a_7'] = x6\n",
    "        features_df.loc[i, 'T3_a_7'] = x7\n",
    "        features_df.loc[i, 'T4_a_7'] = x8\n",
    "        features_df.loc[i, 'T1_b_7'] = x9\n",
    "        features_df.loc[i, 'T2_b_7'] = x10\n",
    "        features_df.loc[i, 'T3_b_7'] = x11\n",
    "        features_df.loc[i, 'T4_b_7'] = x12\n",
    "\n",
    "    for i, j in zip(range(len(n)), n):\n",
    "        x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12 = time_separate(j,7)\n",
    "        features_df.loc[i, 'T1_s_8'] = x1\n",
    "        features_df.loc[i, 'T2_s_8'] = x2\n",
    "        features_df.loc[i, 'T3_s_8'] = x3\n",
    "        features_df.loc[i, 'T4_s_8'] = x4\n",
    "        features_df.loc[i, 'T1_a_8'] = x5\n",
    "        features_df.loc[i, 'T2_a_8'] = x6\n",
    "        features_df.loc[i, 'T3_a_8'] = x7\n",
    "        features_df.loc[i, 'T4_a_8'] = x8\n",
    "        features_df.loc[i, 'T1_b_8'] = x9\n",
    "        features_df.loc[i, 'T2_b_8'] = x10\n",
    "        features_df.loc[i, 'T3_b_8'] = x11\n",
    "        features_df.loc[i, 'T4_b_8'] = x12\n",
    "\n",
    "\n",
    "    #X_train = features_df.ix[:, 1:]\n",
    "    #y_train = features_df.ix[:, 0]\n",
    "\n",
    "    return(features_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/schwarz/.pyenv/versions/3.6.4/lib/python3.6/site-packages/ipykernel_launcher.py:118: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "/Users/schwarz/.pyenv/versions/3.6.4/lib/python3.6/site-packages/ipykernel_launcher.py:120: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n"
     ]
    }
   ],
   "source": [
    "#os.chdir(\"/Applications/SavedData\")\n",
    "#feature_df = get_train(\"aso_train.txt\")\n",
    "list01 = [1,2,0,1,1,0,2,1,2,1,2,0,2,1,0,1,2,0,1,2,1,0,1,0,2,1,0,1,2,1,0,0,1,2,1,0,2,1,0,2,1,2,0,2,1]\n",
    "list02 = [1,2,0,0,2,1,2,1,2,1,0,0,2,1,2,1,2,0,2,1,0,2,1,0,1,2,0,1,0,1,2,1,0,1,0,1,2,1,2,0,0,2,0,1,0]\n",
    "list03 = [1,0,2,1,0,2,2,1,2,0,1,2,0,1,2,1,0,1,0,2,1,0,2,2,1,1,0,0,1,0,2,2,0,1,2,1,2,0,1,2,0,1,2,0,1]\n",
    "\n",
    "feature_df01 = get_train(\"OpenBCI-RAW-ita01.txt\", 6.67, list01)\n",
    "feature_df02 = get_train(\"OpenBCI-RAW-ita02.txt\", 7.14, list02)\n",
    "feature_df03 = get_train(\"OpenBCI-RAW-ita03.txt\", 7.52, list03)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(135, 97)\n"
     ]
    }
   ],
   "source": [
    "train_feature = pd.concat([feature_df01, feature_df02, feature_df03])\n",
    "#train_feature = pd.concat([feature_df01, feature_df02])\n",
    "#train_feature = train_feature.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "\n",
    "print(train_feature.shape)\n",
    "\n",
    "labels = train_feature['labels']\n",
    "labels = np.array(labels.values.flatten())\n",
    "del train_feature['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      T1_s_1    T2_s_1    T3_s_1    T4_s_1    T1_a_1    T2_a_1    T3_a_1  \\\n",
      "0   0.287022  0.009710  0.183148  0.221280  0.103129  0.131193  0.074229   \n",
      "1   0.143683  0.204207  0.191198  0.120523  0.108317  0.153356  0.188031   \n",
      "2   0.106632  0.177912  0.188830  0.048772  0.095701  0.099233  0.091824   \n",
      "3   0.130907  0.211447  0.229594  0.192491  0.061543  0.207152  0.123722   \n",
      "4   0.035323  0.183649  0.096016  0.169982  0.169671  0.124829  0.023756   \n",
      "5   0.183944  0.175736  0.163191  0.085743  0.068292  0.125532  0.122267   \n",
      "6   0.204305  0.047663  0.304170  0.184646  0.097885  0.052545  0.207877   \n",
      "7   0.078792  0.045522  0.177632  0.161968  0.105494  0.159255  0.093698   \n",
      "8   0.247313  0.240297  0.109061  0.239418  0.108072  0.192637  0.160564   \n",
      "9   0.130473  0.175706  0.170644  0.149406  0.122859  0.096841  0.048473   \n",
      "10  0.094395  0.136393  0.212538  0.160344  0.025633  0.041381  0.107289   \n",
      "11  0.093545  0.101595  0.150236  0.216715  0.121153  0.045541  0.136212   \n",
      "12  0.180701  0.141251  0.156678  0.133066  0.061132  0.063436  0.209362   \n",
      "13  0.161684  0.216934  0.144980  0.186918  0.104285  0.097956  0.118126   \n",
      "14  0.210596  0.167458  0.113699  0.119896  0.135107  0.080133  0.106861   \n",
      "15  0.185453  0.252609  0.202488  0.067581  0.110167  0.057947  0.116812   \n",
      "16  0.103716  0.178832  0.202539  0.220737  0.127518  0.185504  0.058962   \n",
      "17  0.165270  0.091217  0.200343  0.180094  0.107077  0.144046  0.266404   \n",
      "18  0.293796  0.200684  0.279690  0.201271  0.126001  0.168628  0.162027   \n",
      "19  0.104671  0.158909  0.128534  0.163189  0.030928  0.033512  0.170573   \n",
      "20  0.071306  0.224426  0.138708  0.154769  0.159781  0.098610  0.143115   \n",
      "21  0.179588  0.172258  0.232491  0.139716  0.145746  0.125281  0.038440   \n",
      "22  0.161998  0.281188  0.017152  0.146972  0.051409  0.075833  0.179287   \n",
      "23  0.090912  0.128274  0.106866  0.151065  0.178028  0.070365  0.072943   \n",
      "24  0.284302  0.146432  0.165307  0.143954  0.029018  0.090092  0.103570   \n",
      "25  0.210119  0.015545  0.167315  0.173279  0.142001  0.063313  0.095933   \n",
      "26  0.102832  0.268488  0.073726  0.217813  0.334272  0.188719  0.096312   \n",
      "27  0.214826  0.182927  0.215404  0.030139  0.106883  0.171359  0.104153   \n",
      "28  0.245928  0.281320  0.116019  0.253607  0.101415  0.047675  0.120724   \n",
      "29  0.179007  0.155700  0.180637  0.153799  0.058064  0.175295  0.073024   \n",
      "..       ...       ...       ...       ...       ...       ...       ...   \n",
      "15  0.113791  0.089806  0.126868  0.114583  0.107436  0.052020  0.115252   \n",
      "16  0.104770  0.106198  0.115668  0.123452  0.118576  0.072740  0.107792   \n",
      "17  0.103259  0.082781  0.100676  0.122310  0.114682  0.070805  0.122272   \n",
      "18  0.144884  0.192332  0.187652  0.120403  0.121220  0.104799  0.116209   \n",
      "19  0.105177  0.133219  0.103450  0.123240  0.109237  0.099083  0.089405   \n",
      "20  0.118186  0.111986  0.215868  0.161285  0.128813  0.124600  0.176270   \n",
      "21  0.179656  0.187037  0.182743  0.164961  0.120723  0.116074  0.131761   \n",
      "22  0.149084  0.175999  0.169635  0.165851  0.105577  0.112401  0.117353   \n",
      "23  0.162934  0.122854  0.136237  0.120206  0.124433  0.097964  0.111908   \n",
      "24  0.121299  0.125491  0.116061  0.119216  0.059520  0.082586  0.123332   \n",
      "25  0.108378  0.115425  0.137093  0.118718  0.113807  0.088749  0.105894   \n",
      "26  0.150929  0.136702  0.112019  0.158726  0.113704  0.092773  0.113234   \n",
      "27  0.179766  0.179385  0.201794  0.222588  0.111315  0.136454  0.136809   \n",
      "28  0.205597  0.121997  0.160148  0.198592  0.110586  0.112459  0.118532   \n",
      "29  0.200824  0.161655  0.135787  0.146967  0.114707  0.073492  0.121971   \n",
      "30  0.132301  0.052459  0.116081  0.123324  0.096553  0.113193  0.101987   \n",
      "31  0.230590  0.164156  0.171973  0.138133  0.127969  0.121508  0.141130   \n",
      "32  0.107869  0.134371  0.058637  0.115797  0.108344  0.087538  0.083323   \n",
      "33  0.142723  0.087431  0.051577  0.102529  0.101179  0.098608  0.077596   \n",
      "34  0.109387  0.104374  0.097262  0.106400  0.115271  0.110331  0.124943   \n",
      "35  0.162075  0.106345  0.138053  0.161652  0.107185  0.115149  0.113996   \n",
      "36  0.118838  0.164102  0.203282  0.076371  0.115630  0.097826  0.087274   \n",
      "37  0.130601  0.100360  0.116316  0.118900  0.113723  0.125182  0.089319   \n",
      "38  0.109757  0.154161  0.097282  0.110786  0.124488  0.069901  0.096014   \n",
      "39  0.127811  0.129069  0.094172  0.100919  0.121000  0.064678  0.105703   \n",
      "40  0.105559  0.103610  0.102324  0.113706  0.095841  0.113088  0.116783   \n",
      "41  0.150883  0.160975  0.070137  0.207446  0.125502  0.120502  0.100576   \n",
      "42  0.200877  0.145010  0.079101  0.075755  0.143448  0.103285  0.119016   \n",
      "43  0.125922  0.125563  0.156608  0.112461  0.090618  0.100299  0.090580   \n",
      "44  0.220428  0.165374  0.376751  0.145493  0.171611  0.115350  0.163827   \n",
      "\n",
      "      T4_a_1    T1_b_1    T2_b_1    ...       T3_s_8    T4_s_8    T1_a_8  \\\n",
      "0   0.033080  0.181379  0.149324    ...     0.113364  0.117334  0.107337   \n",
      "1   0.106139  0.089656  0.108795    ...     0.097026  0.113596  0.115382   \n",
      "2   0.151352  0.096406  0.234131    ...     0.102248  0.117061  0.120797   \n",
      "3   0.131618  0.211967  0.243625    ...     0.115991  0.102974  0.126210   \n",
      "4   0.122088  0.127445  0.091614    ...     0.112349  0.093386  0.102643   \n",
      "5   0.130262  0.164426  0.335406    ...     0.075381  0.108926  0.119676   \n",
      "6   0.097142  0.036237  0.270929    ...     0.107490  0.097998  0.080125   \n",
      "7   0.128602  0.167320  0.308636    ...     0.133297  0.100902  0.125675   \n",
      "8   0.075454  0.161685  0.127778    ...     0.101071  0.121867  0.114654   \n",
      "9   0.340725  0.156287  0.107691    ...     0.106484  0.043674  0.078147   \n",
      "10  0.227603  0.042591  0.071537    ...     0.109820  0.116228  0.114770   \n",
      "11  0.027462  0.257228  0.200104    ...     0.118954  0.101385  0.036865   \n",
      "12  0.112216  0.201792  0.155635    ...     0.067403  0.116250  0.110932   \n",
      "13  0.138188  0.115068  0.257899    ...     0.117025  0.077291  0.094495   \n",
      "14  0.041917  0.119412  0.129506    ...     0.056368  0.120330  0.105727   \n",
      "15  0.050296  0.170373  0.164721    ...     0.101261  0.065687  0.124566   \n",
      "16  0.238477  0.245981  0.130203    ...     0.109115  0.113732  0.118698   \n",
      "17  0.110804  0.153501  0.140189    ...     0.108399  0.098588  0.099269   \n",
      "18  0.121442  0.295416  0.178427    ...     0.107019  0.111737  0.113761   \n",
      "19  0.166158  0.214470  0.144965    ...     0.122644  0.097896  0.088933   \n",
      "20  0.078316  0.261073  0.080201    ...     0.087806  0.121428  0.108710   \n",
      "21  0.166446  0.154291  0.154885    ...     0.117943  0.088004  0.046395   \n",
      "22  0.102480  0.310276  0.234789    ...     0.090127  0.115566  0.107470   \n",
      "23  0.071156  0.202771  0.169770    ...     0.116025  0.035236  0.029678   \n",
      "24  0.065877  0.113239  0.128086    ...     0.095879  0.115702  0.112756   \n",
      "25  0.134250  0.190020  0.083274    ...     0.099376  0.059676  0.089443   \n",
      "26  0.147652  0.175091  0.082586    ...     0.109294  0.109940  0.108265   \n",
      "27  0.084597  0.072734  0.233611    ...     0.113738  0.111246  0.109620   \n",
      "28  0.088686  0.064926  0.143987    ...     0.077501  0.096946  0.098236   \n",
      "29  0.079759  0.214406  0.187759    ...     0.108084  0.096316  0.117473   \n",
      "..       ...       ...       ...    ...          ...       ...       ...   \n",
      "15  0.111485  0.217829  0.252746    ...     0.099485  0.110653  0.112635   \n",
      "16  0.118972  0.250174  0.174242    ...     0.129600  0.167975  0.109641   \n",
      "17  0.113837  0.237165  0.235878    ...     0.089359  0.116040  0.103912   \n",
      "18  0.145496  0.181676  0.175927    ...     0.093330  0.111866  0.111035   \n",
      "19  0.122119  0.232953  0.206312    ...     0.077785  0.098747  0.102129   \n",
      "20  0.092928  0.235127  0.223291    ...     0.120938  0.091291  0.137573   \n",
      "21  0.112893  0.159014  0.176377    ...     0.120298  0.064472  0.112254   \n",
      "22  0.139432  0.144251  0.163540    ...     0.127696  0.095287  0.029165   \n",
      "23  0.091028  0.164740  0.185104    ...     0.110789  0.116753  0.084712   \n",
      "24  0.125671  0.233023  0.170993    ...     0.080092  0.120844  0.098542   \n",
      "25  0.106590  0.239860  0.170179    ...     0.151112  0.125054  0.103035   \n",
      "26  0.104996  0.180953  0.199260    ...     0.107013  0.109973  0.116185   \n",
      "27  0.108464  0.164491  0.145849    ...     0.169178  0.176017  0.085744   \n",
      "28  0.107529  0.134832  0.180222    ...     0.110480  0.105818  0.075510   \n",
      "29  0.106776  0.128465  0.198166    ...     0.111990  0.112807  0.091602   \n",
      "30  0.112108  0.236986  0.225188    ...     0.110655  0.106135  0.123474   \n",
      "31  0.148581  0.179570  0.123935    ...     0.061984  0.112607  0.115686   \n",
      "32  0.115240  0.220073  0.232936    ...     0.093522  0.085260  0.121361   \n",
      "33  0.110324  0.237512  0.246143    ...     0.091454  0.090849  0.118838   \n",
      "34  0.127695  0.223727  0.225565    ...     0.113154  0.006771  0.108894   \n",
      "35  0.131150  0.171538  0.273934    ...     0.121506  0.094103  0.121432   \n",
      "36  0.111238  0.247609  0.178626    ...     0.089078  0.078145  0.114464   \n",
      "37  0.126746  0.239009  0.240103    ...     0.072504  0.132528  0.109534   \n",
      "38  0.105327  0.244573  0.235495    ...     0.045976  0.127259  0.118916   \n",
      "39  0.112884  0.265106  0.205766    ...     0.104977  0.111474  0.108332   \n",
      "40  0.087415  0.197115  0.206307    ...     0.103852  0.107352  0.130179   \n",
      "41  0.112767  0.172402  0.158275    ...     0.120693  0.103332  0.114080   \n",
      "42  0.161402  0.180015  0.221411    ...     0.116196  0.095965  0.102923   \n",
      "43  0.114521  0.245591  0.176892    ...     0.120653  0.074483  0.107595   \n",
      "44  0.117668  0.187862  0.171989    ...     0.124763  0.136808  0.114387   \n",
      "\n",
      "      T2_a_8    T3_a_8    T4_a_8    T1_b_8    T2_b_8    T3_b_8    T4_b_8  \n",
      "0   0.090320  0.120558  0.105280  0.226321  0.233681  0.243147  0.226008  \n",
      "1   0.117434  0.103121  0.091704  0.232053  0.247744  0.265074  0.193041  \n",
      "2   0.108904  0.099026  0.119743  0.240784  0.228139  0.218663  0.232515  \n",
      "3   0.110122  0.126481  0.116693  0.245973  0.231004  0.240902  0.229556  \n",
      "4   0.120504  0.095825  0.070979  0.259934  0.238502  0.236139  0.238673  \n",
      "5   0.105323  0.112000  0.123977  0.231985  0.224147  0.251953  0.254490  \n",
      "6   0.102781  0.107485  0.098294  0.217990  0.252063  0.248346  0.215069  \n",
      "7   0.111752  0.119276  0.107841  0.243405  0.228453  0.249536  0.221650  \n",
      "8   0.040753  0.131264  0.115651  0.227926  0.157093  0.246202  0.251494  \n",
      "9   0.107126  0.110896  0.074732  0.227938  0.227946  0.234579  0.205657  \n",
      "10  0.113176  0.084102  0.125389  0.223809  0.224265  0.214669  0.226543  \n",
      "11  0.122066  0.128838  0.097555  0.274641  0.231337  0.237943  0.211444  \n",
      "12  0.108016  0.086112  0.106997  0.241420  0.236968  0.206678  0.244735  \n",
      "13  0.109097  0.097404  0.093735  0.221275  0.218012  0.244603  0.250434  \n",
      "14  0.111442  0.096573  0.101523  0.248864  0.230112  0.236030  0.222688  \n",
      "15  0.114187  0.122833  0.086370  0.234552  0.238003  0.242456  0.276330  \n",
      "16  0.107496  0.098069  0.108200  0.231305  0.240158  0.212784  0.237745  \n",
      "17  0.118920  0.121742  0.089803  0.245458  0.232054  0.237113  0.233962  \n",
      "18  0.111840  0.088958  0.117217  0.250855  0.234007  0.236161  0.244663  \n",
      "19  0.101937  0.106854  0.101138  0.217580  0.236412  0.246346  0.246572  \n",
      "20  0.121351  0.078391  0.117066  0.249122  0.243413  0.193784  0.240822  \n",
      "21  0.127809  0.104737  0.061612  0.206569  0.239829  0.241878  0.162018  \n",
      "22  0.117635  0.117689  0.111904  0.244940  0.246097  0.235273  0.241169  \n",
      "23  0.118627  0.115517  0.097783  0.283131  0.240208  0.239191  0.209953  \n",
      "24  0.122488  0.106373  0.114124  0.242071  0.212921  0.226169  0.248084  \n",
      "25  0.107805  0.111958  0.021458  0.210312  0.236072  0.250796  0.213111  \n",
      "26  0.098951  0.102919  0.116937  0.250211  0.210643  0.227410  0.229216  \n",
      "27  0.058873  0.109404  0.110016  0.243407  0.255954  0.255723  0.246384  \n",
      "28  0.128220  0.100886  0.121874  0.251004  0.228040  0.242593  0.244734  \n",
      "29  0.111595  0.116238  0.122907  0.234324  0.212220  0.249820  0.241347  \n",
      "..       ...       ...       ...       ...       ...       ...       ...  \n",
      "15  0.122730  0.105402  0.109963  0.237519  0.226372  0.230619  0.252138  \n",
      "16  0.104001  0.071397  0.113768  0.251622  0.204351  0.202810  0.189974  \n",
      "17  0.086519  0.124804  0.112166  0.248519  0.242399  0.239477  0.251634  \n",
      "18  0.107741  0.114853  0.120163  0.232374  0.216889  0.185115  0.235376  \n",
      "19  0.117624  0.086057  0.129154  0.244219  0.239791  0.214255  0.235499  \n",
      "20  0.116163  0.087809  0.098780  0.260039  0.240899  0.186931  0.215443  \n",
      "21  0.111286  0.105646  0.071376  0.188250  0.252726  0.229561  0.213592  \n",
      "22  0.117457  0.099557  0.118038  0.069836  0.227942  0.235533  0.245509  \n",
      "23  0.109767  0.115723  0.112672  0.233662  0.226111  0.237679  0.230990  \n",
      "24  0.071878  0.107910  0.105673  0.253719  0.226514  0.247362  0.244073  \n",
      "25  0.122906  0.106434  0.128879  0.247771  0.183134  0.183828  0.213806  \n",
      "26  0.112520  0.088115  0.063392  0.247337  0.235697  0.240801  0.124379  \n",
      "27  0.112031  0.115504  0.122057  0.146401  0.237407  0.158684  0.169570  \n",
      "28  0.110505  0.113619  0.105912  0.215449  0.219203  0.247922  0.250290  \n",
      "29  0.111284  0.112375  0.118292  0.252889  0.220588  0.226048  0.255238  \n",
      "30  0.109037  0.102833  0.100324  0.230536  0.237474  0.203070  0.257050  \n",
      "31  0.115512  0.069896  0.112748  0.244348  0.241044  0.203425  0.241206  \n",
      "32  0.107075  0.117071  0.091255  0.220187  0.247332  0.231963  0.261419  \n",
      "33  0.118691  0.114808  0.110276  0.242861  0.253096  0.235379  0.243480  \n",
      "34  0.124451  0.125742  0.087293  0.235901  0.235618  0.223168  0.241051  \n",
      "35  0.118799  0.107952  0.094408  0.229482  0.226312  0.212515  0.176774  \n",
      "36  0.109683  0.127176  0.099001  0.245879  0.240537  0.213948  0.257112  \n",
      "37  0.114991  0.045625  0.107376  0.244440  0.248591  0.170816  0.216791  \n",
      "38  0.127579  0.089875  0.116764  0.247921  0.228796  0.260776  0.234422  \n",
      "39  0.095818  0.091086  0.106885  0.254318  0.260635  0.253717  0.240976  \n",
      "40  0.028829  0.125554  0.113827  0.229014  0.270591  0.234822  0.231069  \n",
      "41  0.115846  0.110131  0.125097  0.212665  0.219895  0.238019  0.255576  \n",
      "42  0.119182  0.121101  0.107835  0.175609  0.219903  0.238344  0.181636  \n",
      "43  0.120028  0.099055  0.128919  0.256642  0.239533  0.219807  0.220718  \n",
      "44  0.115867  0.086181  0.103433  0.236409  0.212240  0.170770  0.179558  \n",
      "\n",
      "[135 rows x 96 columns]\n",
      "(135, 96)\n"
     ]
    }
   ],
   "source": [
    "print(train_feature)\n",
    "print(train_feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "累積寄与率: 0.8912489785732567\n",
      "           0         1         2         3         4         5         6   \\\n",
      "90  -0.081945  0.164558  0.069338  0.006667  0.051828  0.003964  0.035650   \n",
      "91  -0.095087 -0.052366 -0.033312  0.060347  0.030277  0.001106 -0.019448   \n",
      "92  -0.097107 -0.062623 -0.018224 -0.044585 -0.097929 -0.030123  0.044702   \n",
      "93   0.009678  0.078680  0.098878 -0.036034  0.112373  0.029585  0.014297   \n",
      "94  -0.050566 -0.016675 -0.185937  0.066305  0.007416  0.013466 -0.035577   \n",
      "95  -0.050920 -0.049192 -0.048836 -0.045797 -0.011948 -0.059822 -0.017725   \n",
      "96  -0.004842  0.098777  0.102120 -0.032478  0.000147  0.048864  0.009777   \n",
      "97  -0.037169  0.005814 -0.071590  0.010201  0.022063 -0.009723 -0.027133   \n",
      "98  -0.105515 -0.057019 -0.017881 -0.018475 -0.019598  0.004640  0.014679   \n",
      "99  -0.115542 -0.043756 -0.006104 -0.027655 -0.053475 -0.030213  0.020554   \n",
      "100 -0.134532  0.000267  0.030302 -0.011835  0.010153 -0.036889 -0.030194   \n",
      "101 -0.099534  0.053352  0.039201  0.002432  0.042181  0.005017 -0.039247   \n",
      "102 -0.115699  0.000521  0.025012 -0.029930  0.030235  0.019299 -0.036290   \n",
      "103 -0.097157  0.025330  0.076493  0.013403  0.041560 -0.015198 -0.003131   \n",
      "104 -0.093606  0.014154  0.104272  0.009143  0.018050 -0.019497 -0.060952   \n",
      "105 -0.097158 -0.026176  0.029591 -0.022084 -0.001583  0.003288 -0.054913   \n",
      "106 -0.062763 -0.016993  0.007206 -0.039547 -0.072443 -0.036060 -0.026264   \n",
      "107 -0.122714 -0.029015  0.020361 -0.003960 -0.021789 -0.017463 -0.043314   \n",
      "108  0.039818 -0.052045 -0.052399 -0.086901 -0.092883 -0.058195  0.058742   \n",
      "109 -0.070495 -0.036923 -0.001121 -0.015581 -0.047734 -0.024938  0.013862   \n",
      "110  0.029358 -0.042313 -0.045087 -0.011745  0.069398 -0.131861  0.023054   \n",
      "111  0.146542 -0.005873 -0.062239  0.026230 -0.004401  0.026785  0.024641   \n",
      "112  0.151634  0.022018  0.035210 -0.015659  0.045639 -0.009455 -0.027764   \n",
      "113 -0.010915  0.048206  0.071271  0.002711  0.029257  0.010303 -0.003733   \n",
      "114 -0.060683  0.032231  0.079257  0.001894 -0.003805 -0.023184 -0.036303   \n",
      "115 -0.032228 -0.022776 -0.001978  0.008744 -0.063687 -0.072585  0.006695   \n",
      "116  0.083852  0.036867 -0.100771  0.036055  0.064676 -0.039263 -0.019610   \n",
      "117  0.177485  0.052856 -0.013109  0.021477 -0.069654  0.029940 -0.033546   \n",
      "118  0.136687 -0.000120  0.036328 -0.016925  0.061080 -0.008287 -0.026979   \n",
      "119  0.090185  0.031494  0.095683 -0.057892  0.028726  0.028642 -0.035951   \n",
      "120 -0.086671 -0.085882  0.021798 -0.027930 -0.017623 -0.021436  0.006884   \n",
      "121  0.080125 -0.056668  0.019684 -0.102706 -0.113990 -0.038235  0.066386   \n",
      "122 -0.030423 -0.060961  0.005466  0.016554  0.029681  0.037079 -0.016620   \n",
      "123 -0.110842 -0.096948  0.001877 -0.020212  0.034926  0.027153 -0.065012   \n",
      "124 -0.031482 -0.008262 -0.078314  0.056901  0.026098  0.011077 -0.009336   \n",
      "125  0.065701  0.009999 -0.109799  0.019583  0.035937  0.030593  0.026073   \n",
      "126  0.019544 -0.141926 -0.057625 -0.006374  0.004585 -0.007731  0.015915   \n",
      "127  0.012173 -0.123086 -0.064406 -0.072151 -0.000857 -0.012504  0.010965   \n",
      "128 -0.084207 -0.078188  0.022501 -0.090238 -0.080992 -0.061658  0.008195   \n",
      "129 -0.043169 -0.092957  0.036517 -0.116203 -0.038661  0.015349 -0.088926   \n",
      "130 -0.016119  0.000239  0.062012 -0.035307  0.055728 -0.011314 -0.024391   \n",
      "131  0.061527  0.033641  0.075891 -0.058541  0.000087 -0.031416 -0.019586   \n",
      "132  0.037502  0.092251 -0.064817  0.035225  0.007084  0.069795 -0.049678   \n",
      "133 -0.026592 -0.050745 -0.031290  0.010274 -0.025410 -0.035542 -0.003680   \n",
      "134  0.186763 -0.038339  0.001642 -0.020480 -0.035081 -0.030997  0.095388   \n",
      "\n",
      "           7         8         9     ...           30        31        32  \\\n",
      "90  -0.015466  0.057936 -0.017706    ...    -0.012907  0.009911  0.009717   \n",
      "91   0.024466  0.008680  0.008219    ...    -0.010076 -0.002446  0.018083   \n",
      "92  -0.036110 -0.019862  0.085893    ...     0.015261  0.005365  0.002143   \n",
      "93   0.041636 -0.003606 -0.005083    ...    -0.009728  0.007414 -0.008137   \n",
      "94   0.042342  0.032187  0.005781    ...     0.000894 -0.016839  0.034739   \n",
      "95  -0.043399 -0.035798  0.035724    ...    -0.002502 -0.021125  0.013523   \n",
      "96  -0.006645 -0.006036  0.014644    ...    -0.034784 -0.007088 -0.017090   \n",
      "97   0.057843 -0.066522 -0.013380    ...     0.003338  0.010920  0.020379   \n",
      "98  -0.002763 -0.027791 -0.014805    ...    -0.004280 -0.007522  0.013169   \n",
      "99  -0.038363 -0.034642  0.117412    ...    -0.006516  0.011126 -0.008231   \n",
      "100 -0.045588  0.054666 -0.003378    ...     0.026401  0.009688 -0.016689   \n",
      "101 -0.041794  0.045476 -0.018841    ...     0.017330  0.012910 -0.011967   \n",
      "102 -0.044211  0.043708 -0.053195    ...     0.005522 -0.020024 -0.007493   \n",
      "103 -0.017142  0.023021  0.001682    ...    -0.020120  0.007914 -0.012185   \n",
      "104 -0.091463  0.086426 -0.049898    ...     0.078470 -0.022305 -0.015741   \n",
      "105 -0.057262 -0.013711  0.021767    ...    -0.007424  0.000514 -0.024714   \n",
      "106 -0.039789  0.029145  0.028546    ...     0.006134  0.004394 -0.024612   \n",
      "107 -0.021047 -0.026595  0.028108    ...    -0.012122 -0.004653 -0.002009   \n",
      "108 -0.023186 -0.056302  0.035334    ...    -0.003544 -0.033376  0.032084   \n",
      "109 -0.010618  0.005851  0.012639    ...    -0.022171 -0.006362  0.009739   \n",
      "110  0.037611 -0.063713  0.005240    ...    -0.023064 -0.008625  0.048804   \n",
      "111  0.040915 -0.065259 -0.047748    ...    -0.026275  0.077685  0.020667   \n",
      "112  0.028220 -0.041627 -0.032212    ...    -0.041008 -0.006438 -0.035246   \n",
      "113 -0.028557  0.018197  0.010242    ...     0.000220  0.022098 -0.015345   \n",
      "114 -0.051547  0.044437 -0.007628    ...    -0.003287 -0.002874 -0.014136   \n",
      "115 -0.027657  0.024936  0.019946    ...    -0.000674 -0.015883 -0.027131   \n",
      "116  0.072464  0.011156 -0.020182    ...    -0.027723 -0.012142 -0.035986   \n",
      "117 -0.008984 -0.064491  0.043425    ...     0.004254  0.063933  0.037553   \n",
      "118  0.005505 -0.033716  0.030285    ...     0.017596 -0.050979  0.011063   \n",
      "119 -0.073178  0.027399  0.014849    ...     0.003779 -0.006403  0.009922   \n",
      "120 -0.052118  0.001275  0.024284    ...    -0.023810  0.043913 -0.017058   \n",
      "121 -0.012618 -0.038221  0.053142    ...    -0.022332  0.011148 -0.048160   \n",
      "122  0.002857  0.025903 -0.022937    ...    -0.000694  0.010248 -0.000440   \n",
      "123  0.020411  0.003435 -0.049259    ...    -0.036885  0.011997  0.017683   \n",
      "124  0.031735  0.002488  0.001277    ...    -0.033482 -0.002016  0.027157   \n",
      "125 -0.011755 -0.004284  0.010064    ...     0.005317 -0.021500 -0.048337   \n",
      "126 -0.086073 -0.093175 -0.108387    ...    -0.015285  0.008199  0.054037   \n",
      "127 -0.019709  0.008265  0.002572    ...     0.031269  0.034671 -0.011802   \n",
      "128 -0.010576  0.006202 -0.031541    ...     0.012837 -0.029463 -0.038031   \n",
      "129 -0.004229  0.045036 -0.085725    ...     0.016355  0.013704 -0.052873   \n",
      "130 -0.001720  0.055905 -0.005932    ...    -0.005468  0.025680 -0.007991   \n",
      "131  0.060599  0.026287 -0.034584    ...     0.030016  0.011698 -0.026334   \n",
      "132  0.074695 -0.067069 -0.010615    ...     0.007074  0.006399  0.028672   \n",
      "133 -0.051344  0.001405 -0.013602    ...    -0.037676 -0.019589  0.008612   \n",
      "134 -0.082730 -0.069363  0.024758    ...    -0.062943  0.021377 -0.019291   \n",
      "\n",
      "           33        34        35        36        37        38        39  \n",
      "90   0.049752 -0.017087  0.013786 -0.046832 -0.027228  0.042335 -0.016650  \n",
      "91   0.006961  0.020011 -0.003123 -0.021155  0.032411  0.015696 -0.037465  \n",
      "92   0.016851 -0.014510 -0.008618 -0.011480  0.011731 -0.008611 -0.019323  \n",
      "93  -0.027949 -0.000816 -0.026614  0.022467  0.019569 -0.006530  0.007061  \n",
      "94   0.029805 -0.023106 -0.009161 -0.025180  0.041253 -0.008570 -0.020886  \n",
      "95  -0.007244  0.011111  0.009455  0.013714 -0.035714 -0.012473  0.004111  \n",
      "96   0.014629 -0.029620  0.000162  0.000835  0.002633 -0.011322  0.006935  \n",
      "97   0.000463 -0.013981 -0.021684 -0.063799  0.001203  0.014348 -0.007645  \n",
      "98  -0.005370  0.018725 -0.023551  0.011007  0.006046  0.012069  0.011358  \n",
      "99   0.012667  0.005910 -0.026016 -0.042590  0.017961  0.015386 -0.003935  \n",
      "100  0.018312 -0.014259 -0.011026 -0.003618 -0.004449 -0.007199 -0.020224  \n",
      "101  0.014398 -0.028720  0.016395  0.008196  0.012582  0.014552  0.008598  \n",
      "102  0.045905 -0.017647  0.007417  0.019805 -0.022446 -0.002065 -0.006952  \n",
      "103  0.026986  0.046357  0.013203  0.011887 -0.016620 -0.035399  0.010513  \n",
      "104  0.026530 -0.014067  0.002581  0.004781 -0.016011 -0.004383 -0.013745  \n",
      "105  0.013903 -0.028595 -0.025550 -0.015665 -0.004364 -0.005333  0.016030  \n",
      "106  0.005744 -0.027093 -0.038963 -0.016143 -0.001877 -0.016857 -0.020956  \n",
      "107 -0.003927 -0.015141 -0.001150 -0.001845 -0.046270  0.013904  0.022955  \n",
      "108  0.026050  0.037789  0.019518 -0.000418  0.023435  0.011111 -0.020567  \n",
      "109 -0.001247  0.009971  0.009566 -0.001631 -0.017522 -0.008642  0.009885  \n",
      "110 -0.006172 -0.025870 -0.016316  0.018068  0.003507 -0.017500 -0.014317  \n",
      "111 -0.003795 -0.009709 -0.001679 -0.001865 -0.024780  0.001331  0.018102  \n",
      "112  0.039799  0.072257  0.023449  0.003728  0.027532 -0.001166 -0.012840  \n",
      "113  0.011207 -0.029991 -0.012213  0.008342  0.013089  0.019352  0.051372  \n",
      "114  0.001946  0.009832  0.012917 -0.002878  0.008691  0.040485  0.007304  \n",
      "115 -0.016567 -0.017813 -0.025657 -0.006685 -0.017427 -0.018927 -0.013695  \n",
      "116  0.003682 -0.006794 -0.010975  0.035313  0.066075 -0.025531  0.001158  \n",
      "117  0.021695 -0.043846  0.010595  0.022258  0.000488 -0.009067 -0.000921  \n",
      "118 -0.073537  0.023347  0.018100  0.012824 -0.013136 -0.018785 -0.031130  \n",
      "119  0.032517  0.014580 -0.016104 -0.025257  0.043586 -0.023410 -0.001078  \n",
      "120  0.000097  0.016759 -0.019321  0.026571 -0.020682  0.004228  0.004050  \n",
      "121  0.014520  0.041103 -0.015896  0.029195  0.024718  0.002920 -0.010274  \n",
      "122 -0.002651  0.032916  0.003961 -0.021188  0.038180  0.011618  0.029414  \n",
      "123 -0.040590  0.009319 -0.012798 -0.005238 -0.010223  0.003553  0.007070  \n",
      "124  0.031704  0.018851 -0.014814 -0.037431 -0.007092  0.016550  0.032535  \n",
      "125  0.019664 -0.028594 -0.027656 -0.018111  0.008632 -0.018451  0.032829  \n",
      "126  0.011479  0.017318  0.003064  0.027496  0.022627  0.027424 -0.003544  \n",
      "127 -0.006619  0.011332  0.001004  0.035241  0.040222  0.012256  0.009209  \n",
      "128  0.018867 -0.013509  0.001297  0.045033 -0.027871 -0.022017 -0.003899  \n",
      "129 -0.013971 -0.006569 -0.008126 -0.008493 -0.031459 -0.001014  0.003253  \n",
      "130  0.009326  0.001927 -0.004428  0.023152  0.013185  0.047891  0.026454  \n",
      "131  0.007930 -0.003269  0.039088 -0.044921 -0.034404  0.005966  0.006649  \n",
      "132  0.033976  0.024207 -0.067168 -0.016390  0.018751  0.029356 -0.049735  \n",
      "133  0.004156 -0.002910 -0.004191 -0.005677  0.018746  0.005414 -0.005633  \n",
      "134 -0.001341 -0.071866 -0.006813 -0.049353  0.014404 -0.047967 -0.042170  \n",
      "\n",
      "[45 rows x 40 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/schwarz/.pyenv/versions/3.6.4/lib/python3.6/site-packages/ipykernel_launcher.py:12: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "#pca = PCA(n_components=45)\n",
    "pca = PCA(n_components=40)\n",
    "pca.fit(train_feature)\n",
    "train_feature = pca.transform(train_feature)\n",
    "#train_feature = pca.transform(pd.concat([feature_df01, feature_df02, feature_df03]))\n",
    "print('累積寄与率: {0}'.format(sum(pca.explained_variance_ratio_)))\n",
    "\n",
    "train_feature = pd.DataFrame(train_feature)\n",
    "\n",
    "#X_train = train_feature.ix[:89, :]\n",
    "#y_train = labels[:90]\n",
    "X_train = train_feature.ix[:89, :]\n",
    "y_train = labels[:90]\n",
    "\n",
    "#X_test1 = train_feature.ix[90:104, :]\n",
    "#y_test1 = labels[90:105]\n",
    "X_test1 = train_feature.ix[90:104+30, :]\n",
    "y_test1 = labels[90:105+30]\n",
    "#X_test1 = X_train\n",
    "#y_test1 = y_train\n",
    "print(X_test1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "累積寄与率: 1.0\n",
      "0.35555555555555557\n"
     ]
    }
   ],
   "source": [
    "lda = LDA(n_components=2)\n",
    "lda.fit(X_train, y_train)\n",
    "X_train = lda.transform(X_train)\n",
    "X_test1 = lda.transform(X_test1)\n",
    "\n",
    "print('累積寄与率: {0}'.format(sum(lda.explained_variance_ratio_)))\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=200, min_samples_split=15, max_depth=3, max_features=None)\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "w1 = f1_score(y_test1, forest.predict(X_test1),average = 'micro')\n",
    "\n",
    "\n",
    "print(w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 0 1 1 0 2 1 2 1 2 0 2 1 0 1 2 0 1 2 1 0 1 0 2 1 0 1 2 1 0 0 1 2 1 0 2\n",
      " 1 0 2 1 2 0 2 1 1 2 0 0 2 1 2 1 2 1 0 0 2 1 2 1 2 0 2 1 0 2 1 0 1 2 0 1 0\n",
      " 1 2 1 0 1 0 1 2 1 2 0 0 2 0 1 0]\n",
      "[1 2 0 2 1 0 2 1 2 1 2 0 2 1 0 2 1 0 1 2 1 0 1 0 0 1 0 1 2 1 0 0 0 2 1 1 2\n",
      " 1 0 1 1 2 0 2 1 1 2 0 0 2 1 2 1 2 1 0 0 2 1 2 1 2 0 1 1 0 1 1 0 1 2 0 1 2\n",
      " 1 2 1 0 1 0 1 2 0 2 0 0 2 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)\n",
    "print(forest.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[25,  2,  1],\n",
       "       [ 2, 30,  2],\n",
       "       [ 1,  4, 23]])"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_train,forest.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 2 1 0 2 2 1 2 0 1 2 0 1 2 1 0 1 0 2 1 0 2 2 1 1 0 0 1 0 2 2 0 1 2 1 2\n",
      " 0 1 2 0 1 2 0 1]\n",
      "[2 0 2 2 0 2 2 0 0 0 2 2 2 1 2 2 2 2 2 0 0 0 1 1 2 2 0 0 1 2 2 1 0 0 0 0 0\n",
      " 1 2 1 2 1 0 1 2]\n"
     ]
    }
   ],
   "source": [
    "print(y_test1)\n",
    "print(forest.predict(X_test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test1,forest.predict(X_test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_cm = cm[0][0]+cm[1][1]+cm[2][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (tr_cm)/(cm[0][0] + cm[0][1] + cm[0][2] + cm[1][0] + cm[1][1] + cm[1][2] + cm[2][0] + cm[2][1] + cm[2][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35555555555555557\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
